{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "CR_Challenge.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f1ec663cf9f34be396a74c81622f6707": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_65e6f768494245ec8f777a70d56d5d1a",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a66d7ebfef4a4f64bc6c0dcc34d730d1",
              "IPY_MODEL_84c05740418d4cb1849e4ad3f196ec90"
            ]
          }
        },
        "65e6f768494245ec8f777a70d56d5d1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a66d7ebfef4a4f64bc6c0dcc34d730d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_6d419589d29943b888636170492e18ee",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 553433881,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 553433881,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_c86cf28af1ed4792a9f8e6a0bd676d2c"
          }
        },
        "84c05740418d4cb1849e4ad3f196ec90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7dc87b0dece347c689906936c0c838c8",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 528M/528M [00:13&lt;00:00, 42.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_55d9c2649449477da73be4a209525d37"
          }
        },
        "6d419589d29943b888636170492e18ee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c86cf28af1ed4792a9f8e6a0bd676d2c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7dc87b0dece347c689906936c0c838c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "55d9c2649449477da73be4a209525d37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML4SCI/ML4SCIHackathon/blob/main/CosmicRayImagesChallenge/CR_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-C40d9p6Bqg"
      },
      "source": [
        "# Hackathon - Cosmic Ray Detection Challenge"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gs8EcFUP6Bql"
      },
      "source": [
        "## 1. Problem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hpp_qFW7XTsH"
      },
      "source": [
        "### 1.1 Description"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L1kqKmSc6Bqo"
      },
      "source": [
        "- CCD-based cameras are used extensively in exploration satellites and space telescopes for imaging the surfaces of celestial bodies, and deep-space objects such as stars, extrasolar systems, and galaxies. \n",
        "\n",
        "- These sensors are often subjected to constant irradiation by galactic cosmic rays. The interaction of these ionizing radiations with optical sensors leads to confusion and loss of imaging pixels (an â€˜artifactâ€™).\n",
        "\n",
        "- Your goal is to develop a model which automatically locates cosmic ray artifacts by creating bounding boxes around them\n",
        " "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOg1fH2R6Bqt"
      },
      "source": [
        "### 1.2 Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMSEpjPse2Rb"
      },
      "source": [
        "----------------------\n",
        "    Path to dataset\n",
        "                root\n",
        "         /     /            \\      \\ \n",
        "    train train_annots  valid  valid_annots\n",
        "\n",
        "  -----------------------------\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQ2a7RzXna9I",
        "outputId": "bc96808a-7356-4b87-ceb5-1cfccf139897",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 322
        }
      },
      "source": [
        "# Download the train and validation data (images and annotations)\n",
        "!wget 'https://drive.google.com/uc?export=download&id=1lO7tdqYtO1arEzLB9RkUrF7efxkyLPML' -O root.zip\n",
        "!unzip root.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-10-23 18:24:05--  https://drive.google.com/uc?export=download&id=1lO7tdqYtO1arEzLB9RkUrF7efxkyLPML\n",
            "Resolving drive.google.com (drive.google.com)... 74.125.142.102, 74.125.142.113, 74.125.142.139, ...\n",
            "Connecting to drive.google.com (drive.google.com)|74.125.142.102|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Moved Temporarily\n",
            "Location: https://doc-0g-78-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3ju1dkqj8bjnvdchjesave0k45brohbh/1603477425000/08529322520504532382/*/1lO7tdqYtO1arEzLB9RkUrF7efxkyLPML?e=download [following]\n",
            "Warning: wildcards not supported in HTTP.\n",
            "--2020-10-23 18:24:09--  https://doc-0g-78-docs.googleusercontent.com/docs/securesc/ha0ro937gcuc7l7deffksulhg5h7mbp1/3ju1dkqj8bjnvdchjesave0k45brohbh/1603477425000/08529322520504532382/*/1lO7tdqYtO1arEzLB9RkUrF7efxkyLPML?e=download\n",
            "Resolving doc-0g-78-docs.googleusercontent.com (doc-0g-78-docs.googleusercontent.com)... 74.125.142.132, 2607:f8b0:400e:c08::84\n",
            "Connecting to doc-0g-78-docs.googleusercontent.com (doc-0g-78-docs.googleusercontent.com)|74.125.142.132|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: unspecified [application/x-zip-compressed]\n",
            "Saving to: â€˜root.zipâ€™\n",
            "\n",
            "root.zip                [ <=>                ]  19.33M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2020-10-23 18:24:09 (200 MB/s) - â€˜root.zipâ€™ saved [20269114]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLIlkWNUtE1t"
      },
      "source": [
        "# Download the test data (images only)\n",
        "!wget 'https://drive.google.com/uc?export=download&id=15U7EI4ywSraUhFb5VBPUvQX5dGm6uzNS' -O test_images.zip\n",
        "!unzip test_images.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1FCAAlGW6Bqu"
      },
      "source": [
        "#### **Observe, the size of the dataset is what makes the problem interesting!**<br>\n",
        "You don't have a lot of data, thus your goal is to maximize learning on this scarce data.<br>\n",
        "Wonder how you can leverage your Deep learning skills to obtain decent results for the problem!\n",
        "<br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRzvdZKKVD3R"
      },
      "source": [
        "## 2.  Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjG5Ef9QXZnX"
      },
      "source": [
        "### 2.1 Some pointers to help you start!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KFx8OcIi6Bqw"
      },
      "source": [
        "1. Browse about existing techniques like few shot learning, and how they are used when the size of datasets is small.\n",
        "2. Try to learn about object detectors based on backbones like ResNet, which may be efficient in locating small objects.\n",
        "\n",
        "Feel free to come up with other interesting ideas!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvjDQnVpVWjP"
      },
      "source": [
        "Below, we present an example of training a model called 'Region Proposal Network (RPN) with VGG'. <br>\n",
        "This approach is just meant to give you an idea about the problem. <br>\n",
        "You are highly encouraged to try other machine learning models for solving the problem. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_RtZZoKZ9WFN"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from numpy import asarray\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "import xml.etree.ElementTree as ET\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "from torchvision.datasets.voc import VisionDataset\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import torch.utils.model_zoo as model_zoo"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "if8Jy5sSCiW-",
        "outputId": "7a827cf0-62d8-4b97-9c63-58426be4c7bb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7MbkED_xWnKW"
      },
      "source": [
        "### 2.2 Dataset pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h3nTo-5E-f2a"
      },
      "source": [
        "def generate_anchors(target): # data is the training set\n",
        "  gt_bbox = np.array(target[0],dtype=np.float32)\n",
        "  gt_labels = np.array(target[1])\n",
        "  anchor_areas = [ x**2 for x in [16.0, 32.0, 64.0]]\n",
        "  ratios = [0.5, 1, 2]\n",
        "  anchor_scales = [1]\n",
        "  fe_size = 32\n",
        "  anchors = np.zeros((len(anchor_areas)*len(ratios)*len(anchor_scales)*fe_size*fe_size, 4),dtype=np.float32)\n",
        "  ctr =  np.zeros((fe_size*fe_size, 2),dtype=np.float32)\n",
        "  pts = np.arange(1024/fe_size-1, 1024, 1024/fe_size)\n",
        "  index = 0\n",
        "  for x in range(len(pts)):\n",
        "      for y in range(len(pts)):\n",
        "          ctr[index, 0] = pts[x] - 1024/(fe_size*2)\n",
        "          ctr[index, 1] = pts[y] - 1024/(fe_size*2)\n",
        "          index +=1\n",
        "\n",
        "  # Code to generate anchors at a location in Fmap\n",
        "  index = 0\n",
        "  for c in ctr:\n",
        "    ctr_y, ctr_x = c\n",
        "    for i in range(len(ratios)):\n",
        "      for j in range(len(anchor_scales)):\n",
        "        for k in range(len(anchor_areas)):\n",
        "          h = anchor_scales[j] * np.sqrt(anchor_areas[k]/ratios[i])\n",
        "          w = anchor_scales[j] * (anchor_areas[k]/h)\n",
        "          anchors[index, 0] = ctr_y - h / 2.\n",
        "          anchors[index, 1] = ctr_x - w / 2.\n",
        "          anchors[index, 2] = ctr_y + h / 2.\n",
        "          anchors[index, 3] = ctr_x + w / 2.\n",
        "          index += 1\n",
        "  valid_anchors = anchors\n",
        "  valid_anchors[:, slice(0, 4, 2)] = np.clip(\n",
        "              anchors[:, slice(0, 4, 2)], 0, 1024)\n",
        "  valid_anchors[:, slice(1, 4, 2)] = np.clip(\n",
        "      anchors[:, slice(1, 4, 2)], 0, 1024)\n",
        "  # Create labels and anchors for valid anchor boxes\n",
        "  label = np.empty((len(valid_anchors), ), dtype=np.int32)\n",
        "  # default initialisation\n",
        "  label.fill(-1)\n",
        "\n",
        "  if (len(gt_labels)==0):\n",
        "    anchor_locations = np.empty((len(anchors),) + anchors.shape[1:], dtype=anchors.dtype)\n",
        "    anchor_locations.fill(0)\n",
        "    return label,anchor_locations\n",
        "  ious = np.empty((len(valid_anchors), len(gt_labels)), dtype=np.float32)\n",
        "  ious.fill(0)\n",
        "\n",
        "# Calculating iou\n",
        "  for num1, i in enumerate(valid_anchors):\n",
        "      ya1, xa1, ya2, xa2 = i  \n",
        "      anchor_area = (ya2 - ya1) * (xa2 - xa1)\n",
        "      for num2, j in enumerate(gt_bbox):\n",
        "          yb1, xb1, yb2, xb2 = j\n",
        "          box_area = (yb2- yb1) * (xb2 - xb1)\n",
        "          inter_x1 = max([xb1, xa1])\n",
        "          inter_y1 = max([yb1, ya1])\n",
        "          inter_x2 = min([xb2, xa2])\n",
        "          inter_y2 = min([yb2, ya2])\n",
        "          if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
        "              iter_area = (inter_y2 - inter_y1)*(inter_x2 - inter_x1)\n",
        "              iou = iter_area/(anchor_area+ box_area - iter_area)            \n",
        "          else:\n",
        "              iou = 0\n",
        "          ious[num1, num2] = iou\n",
        "\n",
        "  gt_argmax_ious = ious.argmax(axis=0)\n",
        "  gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
        "  argmax_ious = ious.argmax(axis=1)\n",
        "  max_ious = ious[np.arange(len(label)), argmax_ious]\n",
        "\n",
        "  # LABELLING GT ANCHORS\n",
        "  pos_iou_threshold  = 0.5\n",
        "  neg_iou_threshold = 0.05\n",
        "  label[max_ious < neg_iou_threshold] = 0\n",
        "  label[gt_argmax_ious] = 1\n",
        "  label[max_ious >= pos_iou_threshold] = 1\n",
        "\n",
        "  # Creating 256 training samples,half positive, half negative\n",
        "  # pos_ratio = 0.5\n",
        "  n_neg = 1024\n",
        "  pos_index = np.where(label==1)[0]\n",
        "  neg_index = np.where(label == 0)[0]\n",
        "  if len(neg_index) > n_neg:\n",
        "      disable_index = np.random.choice(neg_index, size=(len(neg_index) - n_neg), replace = False)\n",
        "      label[disable_index] = -1\n",
        "\n",
        "  final_neg_anchor_indices = np.where(label==0)[0]\n",
        "  final_neg_anchors = valid_anchors[final_neg_anchor_indices]\n",
        "# get the most probable gt_bbox at each valid anchor location \n",
        "  max_iou_bbox = gt_bbox[argmax_ious]\n",
        "\n",
        "  height = valid_anchors[:, 2] - valid_anchors[:, 0]\n",
        "  width = valid_anchors[:, 3] - valid_anchors[:, 1]\n",
        "  ctr_y = valid_anchors[:, 0] + 0.5 * height\n",
        "  ctr_x = valid_anchors[:, 1] + 0.5 * width\n",
        "  base_height = max_iou_bbox[:, 2] - max_iou_bbox[:, 0]\n",
        "  base_width = max_iou_bbox[:, 3] - max_iou_bbox[:, 1]\n",
        "  base_ctr_y = max_iou_bbox[:, 0] + 0.5 * base_height\n",
        "  base_ctr_x = max_iou_bbox[:, 1] + 0.5 * base_width\n",
        "  eps = np.finfo(height.dtype).eps\n",
        "  height = np.maximum(height, eps)\n",
        "  width = np.maximum(width, eps)\n",
        "  dy = (base_ctr_y - ctr_y) / height\n",
        "  dx = (base_ctr_x - ctr_x) / width\n",
        "  dh = np.log(base_height / height)\n",
        "  dw = np.log(base_width / width)\n",
        "  anchor_locs = np.vstack((dy, dx, dh, dw)).transpose()\n",
        "  anchor_locations = np.empty((len(anchors),) + anchors.shape[1:], dtype=anchor_locs.dtype)\n",
        "  anchor_locations.fill(0)\n",
        "  anchor_locations = anchor_locs\n",
        "  # print(label.shape,anchor_locations.shape)\n",
        "  return label,anchor_locations\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwxorFvZ6Bqz"
      },
      "source": [
        "# data loader\n",
        "data_transforms = {\n",
        "    'train': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ]),\n",
        "    'valid': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])\n",
        "    ]),\n",
        "}\n",
        "class Cosmic_Detection(VisionDataset):\n",
        "  def __init__(self,\n",
        "                root,                 \n",
        "                mode='train',\n",
        "                transform=None,\n",
        "                target_transform=None,\n",
        "                transforms=None):\n",
        "      super(Cosmic_Detection, self).__init__(root, transforms, transform, target_transform)\n",
        "\n",
        "      voc_root = 'root'  # Name of your dataset folder \n",
        "      image_dir = os.path.join(voc_root, mode)\n",
        "      annotation_dir = os.path.join(voc_root, mode +'_annots')\n",
        "\n",
        "      if not os.path.isdir(voc_root):\n",
        "          raise RuntimeError('Dataset not found or corrupted')\n",
        "\n",
        "      file_names = []\n",
        "      for f in os.listdir(image_dir):\n",
        "        file_names.append(f[:-4])\n",
        "      self.images = [os.path.join(image_dir, x + \".jpg\") for x in file_names]\n",
        "      self.annotations = [os.path.join(annotation_dir, x + \".xml\") for x in file_names]\n",
        "      assert (len(self.images) == len(self.annotations))\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "      img = Image.open(self.images[index]).convert('RGB')\n",
        "      target = self.parse_voc_xml(ET.parse(self.annotations[index]).getroot())\n",
        "\n",
        "      if self.transforms is not None:\n",
        "          img, target = self.transforms(img, target)\n",
        "\n",
        "      labels, locations = generate_anchors(target)\n",
        "\n",
        "      return img.view(1,3,1024,1024), labels, locations\n",
        "\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.images)\n",
        "\n",
        "    \n",
        "  def parse_voc_xml(self, root):\n",
        "        # extract each bounding box\n",
        "        boxes = list()\n",
        "        for box in root.findall('.//bndbox'):\n",
        "            xmin = int(box.find('xmin').text)\n",
        "            ymin = int(box.find('ymin').text)\n",
        "            xmax = int(box.find('xmax').text)\n",
        "            ymax = int(box.find('ymax').text)\n",
        "            coors = [ymin, xmin, ymax, xmax]\n",
        "            boxes.append(coors)\n",
        "        # extract image dimensions\n",
        "        width = int(root.find('.//size/width').text)\n",
        "        height = int(root.find('.//size/height').text)\n",
        "        # extract all the classes\n",
        "        labels = list()\n",
        "        for category in root.findall('.//name'):\n",
        "          labels.append(category.text)\n",
        "        return boxes, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cQz75QoOElgy"
      },
      "source": [
        "dataset = {x: Cosmic_Detection(None, x, data_transforms[x]) \n",
        "                  for x in ['train', 'valid']}\n",
        "\n",
        "dataloaders = {x: torch.utils.data.DataLoader(dataset[x], batch_size=1,\n",
        "                                             shuffle=True, num_workers=4, pin_memory = True)\n",
        "              for x in ['train', 'valid']}\n",
        "\n",
        "dataset_sizes = {x: len(dataset[x]) for x in ['train', 'valid']}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pl3Yy27-EX-5",
        "outputId": "5003ad8c-96d2-4c6b-f85c-3d594a14dd33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "for inputs,labels,locs in dataloaders['train']:\n",
        "        print(inputs[0].shape)\n",
        "        print(labels[0].shape)\n",
        "        print(locs[0].shape)\n",
        "        break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([1, 3, 1024, 1024])\n",
            "torch.Size([9216])\n",
            "torch.Size([9216, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDvcmFvbW9oB"
      },
      "source": [
        "### 2.3 Defining Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RtlOjWSy_7T4",
        "outputId": "d13926de-2909-44ea-c176-de023787fc33",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119,
          "referenced_widgets": [
            "f1ec663cf9f34be396a74c81622f6707",
            "65e6f768494245ec8f777a70d56d5d1a",
            "a66d7ebfef4a4f64bc6c0dcc34d730d1",
            "84c05740418d4cb1849e4ad3f196ec90",
            "6d419589d29943b888636170492e18ee",
            "c86cf28af1ed4792a9f8e6a0bd676d2c",
            "7dc87b0dece347c689906936c0c838c8",
            "55d9c2649449477da73be4a209525d37"
          ]
        }
      },
      "source": [
        "VGG_model = torchvision.models.vgg16(pretrained=True)\n",
        "fe = list(VGG_model.features)\n",
        "mid_channels = 512\n",
        "in_channels = 512 # depends on the output feature map. in vgg 16 it is equal to 512\n",
        "n_anchor = 9 # Number of anchors at each location\n",
        "conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1)\n",
        "# Initialising the layer weights with values from Gaussian(0,0.01)\n",
        "# conv sliding layer\n",
        "conv1.weight.data.normal_(0, 0.01)\n",
        "conv1.bias.data.zero_()\n",
        "print(fe)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f1ec663cf9f34be396a74c81622f6707",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=553433881.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S65jJMf9_32t"
      },
      "source": [
        "class Detector(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Detector, self).__init__()\n",
        "        self.backbone = torch.nn.Sequential(*fe,conv1)\n",
        "        self.reg_layer = nn.Conv2d(mid_channels, n_anchor *4, 1, 1, 0)\n",
        "        self.cls_layer = nn.Conv2d(mid_channels, n_anchor *2, 1, 1, 0)\n",
        "        # Regression layer\n",
        "        self.reg_layer.weight.data.normal_(0, 0.01)\n",
        "        self.reg_layer.bias.data.zero_()\n",
        "        # classification layer\n",
        "        self.cls_layer.weight.data.normal_(0, 0.01)\n",
        "        self.cls_layer.bias.data.zero_()\n",
        "        \n",
        "    def forward(self, input_image):\n",
        "        out_map = self.backbone(input_image)\n",
        "        pred_cls_scores = self.cls_layer(out_map)\n",
        "        pred_anchor_locs = self.reg_layer(out_map)\n",
        "        return pred_cls_scores, pred_anchor_locs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "54YdeLWNCZGl"
      },
      "source": [
        "CR_Detector = Detector().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmtyPFbyZfHp"
      },
      "source": [
        "### 2.4 Example Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TiW-gHRTC68u"
      },
      "source": [
        "def train_model(model, start_epoch = 0,num_epochs=1):\n",
        "  weights = [0.05, 0.95]\n",
        "  class_weights = torch.FloatTensor(weights).to(device)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "  since = time.time()\n",
        "  model_wts = copy.deepcopy(model.state_dict())\n",
        "  best_loss = 100.0\n",
        "        \n",
        "  for epoch in range(num_epochs):        \n",
        "      print('Epoch {}/{}'.format(epoch + start_epoch, start_epoch+ num_epochs-1))\n",
        "      print('-' * 10)\n",
        "\n",
        "      # Each epoch has a training and validation phase\n",
        "      for phase in ['train', 'valid']:\n",
        "          if phase == 'train':\n",
        "              model.train()  # Set model to training mode\n",
        "          else:\n",
        "              model.eval()   # Set model to evaluate mode\n",
        "                \n",
        "          cls_loss = 0.0\n",
        "          reg_loss = 0.0  \n",
        "          running_loss = 0.0            \n",
        "          total_cls_loss = 0.0\n",
        "          total_reg_loss = 0.0\n",
        "          total_epoch_loss = 0.0\n",
        "          index = -1\n",
        "\n",
        "          # Iterate over data.\n",
        "          for inputs, labels, locations in dataloaders[phase]:\n",
        "            # Batch size 1\n",
        "              index+=1\n",
        "              inputs = inputs[0].to(device)\n",
        "              target_labels_np = labels[0].data.numpy()\n",
        "              pos_arr = np.where(target_labels_np > 0)[0]\n",
        "              target_labels = labels[0].to(device)\n",
        "              target_locs = locations[0].to(device)\n",
        "              pos = target_labels > 0\n",
        "              # zero the parameter gradients\n",
        "              optimizer.zero_grad()\n",
        "              # forward\n",
        "              # track history if only in train\n",
        "              with torch.set_grad_enabled(phase == 'train'):\n",
        "                  pred_labels, pred_locs = model(inputs)\n",
        "                  if torch.isnan(pred_locs ).any():\n",
        "                      print(\"Nan in output prediction:\", index)\n",
        "                      return None\n",
        "\n",
        "                  pred_labels = pred_labels.permute(0, 2, 3, 1).contiguous()\n",
        "                  pred_labels = pred_labels.view(1, 32, 32, 9, 2).contiguous().view(-1, 2)\n",
        "                  pred_locs = pred_locs.permute(0, 2, 3, 1).contiguous().view(-1, 4)\n",
        "                  rpn_cls_loss = F.cross_entropy(pred_labels, target_labels.long(), ignore_index = -1,weight = class_weights)\n",
        "                \n",
        "                  if not(pos_arr.size == 0):\n",
        "                      mask = pos.unsqueeze(1).expand_as(pred_locs)\n",
        "                      mask_loc_preds = pred_locs[mask].view(-1,4)\n",
        "                      mask_loc_targets = target_locs[mask].view(-1, 4)\n",
        "                      x = torch.abs(mask_loc_targets - mask_loc_preds)\n",
        "                      rpn_loc_loss = (((x < 1).float() * 0.5 * x*x) + ((x >= 1).float() * (x-0.5))).sum()\n",
        "                      rpn_loc_loss_item = rpn_loc_loss.item()\n",
        "                      rpn_loss = rpn_cls_loss +  rpn_loc_loss\n",
        "                  else:\n",
        "                      rpn_loc_loss_item = 0\n",
        "                      rpn_loss = rpn_cls_loss\n",
        "                  \n",
        "                  # backward + optimize only if in training phase\n",
        "                  if phase == 'train':\n",
        "                      rpn_loss.backward()\n",
        "                      optimizer.step()\n",
        "\n",
        "              # statistics\n",
        "              cls_loss += rpn_cls_loss.item()\n",
        "              reg_loss += rpn_loc_loss_item\n",
        "              running_loss += rpn_loss.item()\n",
        "          # if phase == 'train':\n",
        "          #     scheduler.step()\n",
        "\n",
        "          total_cls_loss = cls_loss / dataset_sizes[phase]\n",
        "          total_reg_loss = reg_loss / dataset_sizes[phase]\n",
        "          total_epoch_loss = running_loss / dataset_sizes[phase]\n",
        "\n",
        "          print('{} rpn_cls Loss: {:.4f}       {} rpn_reg Loss: {:.4f}       {} Total Loss: {:.4f}'.format(phase, total_cls_loss,phase, total_reg_loss,phase, total_epoch_loss))\n",
        "\n",
        "          # deep copy the model\n",
        "          if phase == 'train' and total_epoch_loss < best_loss:\n",
        "              best_loss = total_epoch_loss\n",
        "              model_wts = copy.deepcopy(model.state_dict())\n",
        "      print()\n",
        "  \n",
        "  time_elapsed = time.time() - since\n",
        "  print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "      time_elapsed // 60, time_elapsed % 60))\n",
        "  print('Best Loss: {:4f}'.format(best_loss))\n",
        "\n",
        "  # load model weights\n",
        "  model.load_state_dict(model_wts)\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4BT2A0SDU1m",
        "outputId": "1e9fb595-1dd8-445d-e29f-86c260a7a1c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "# Example Training for 1 epoch\n",
        "model_1 = train_model(CR_Detector)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/0\n",
            "----------\n",
            "train rpn_cls Loss: 0.3594       train rpn_reg Loss: 0.5548       train Total Loss: 0.9142\n",
            "valid rpn_cls Loss: 0.1589       valid rpn_reg Loss: 0.4608       valid Total Loss: 0.6196\n",
            "\n",
            "Training complete in 1m 52s\n",
            "Best Loss: 0.914225\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbADW8Js6Bq9"
      },
      "source": [
        "## 3. Evaluation and Submission"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcF4LTaaZq1s"
      },
      "source": [
        "### 3.1 Evaluation Metric"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zd0h4bZCY4BH"
      },
      "source": [
        "- You will be evaluated based on the following criteria:\n",
        "  - **Mean Average Precision (MAP) score** of your model on the test data\n",
        "\n",
        "<!-- - You are encouraged to implement the evaluation module yourself. If not, there are tons of repositories to help you which provide you the implementation of these metrics: Just google 'MAP Object detection git' -->\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RA94yVjEYv0T"
      },
      "source": [
        "### 3.2 Submission Format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiRDG2xm6Bq_"
      },
      "source": [
        "You need to submit the predictions of your model on the test data as follows:\n",
        "\n",
        "- For each image in the test folder, you need to write your predictions into a corresponding \".txt\" file with the same image name, following the format below:<br>\n",
        "   ```Positive  <confidence> <left> <top> <right> <bottom>```\n",
        "    - Class name - Positive (will be constant across all predictions)\n",
        "    - Confidence score for that prediction (between 0 and 1)\n",
        "    - left x co-ordinate of the box predicted (between 0 and 1024)\n",
        "    - top y co-ordinate of the box predicted (between 0 and 1024)\n",
        "    - right x co-ordinate of the box predicted (between 0 and 1024)\n",
        "    - bottom y co-ordinate of the box predicted (between 0 and 1024)\n",
        "    \n",
        "- An image may contain more than 1 artifact, in which case each prediction instance is to be written on a separate line of the '.txt' file.\n",
        "- Put all these '.txt' files into a single folder with the name ```predictions/``` \n",
        "- Your submission should include the ```notebook``` along with the ```predictions/``` folder. Zip them into ```<Your Team Number>.zip``` before submission.\n",
        "    \n",
        "    \n",
        "    \n",
        "\n"
      ]
    }
  ]
}
